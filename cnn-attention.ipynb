{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.utils import to_categorical\n\n\n# Load CIFAR-10 dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n# Normalize the data\nx_train = x_train.astype('float32') / 255.0\nx_test = x_test.astype('float32') / 255.0\n\n# Convert class vectors to binary class matrices (one-hot encoding)\ny_train = to_categorical(y_train, 10)\ny_test = to_categorical(y_test, 10)\n\n# Load the ResNet50 model with ImageNet weights, excluding the top layer\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n\n# Set all layers to be trainable\nfor layer in base_model.layers:\n    layer.trainable = True\n\n# Add custom layers on top of the base model\nx = base_model.output\nx = Flatten()(x)\nx = Dense(512, activation='relu')(x)\npredictions = Dense(10, activation='softmax')(x)\n\n# Define the model\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n\n# Evaluate the model\ntest_loss, test_accuracy = model.evaluate(x_test, y_test)\nprint(f\"Test accuracy: {test_accuracy}\")\n\n# Predict labels for test set\npredictions = model.predict(x_test)\npredicted_labels = tf.argmax(predictions, axis=1)\nactual_labels = tf.argmax(y_test, axis=1)\n\n# Print overall accuracy\naccuracy = tf.reduce_mean(tf.cast(predicted_labels == actual_labels, tf.float32))\nprint(f\"Overall accuracy: {accuracy.numpy()}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-20T16:27:12.024193Z","iopub.execute_input":"2024-05-20T16:27:12.025320Z","iopub.status.idle":"2024-05-20T16:36:34.432505Z","shell.execute_reply.started":"2024-05-20T16:27:12.025276Z","shell.execute_reply":"2024-05-20T16:36:34.431558Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ImageNet is an image database organized according to the WordNet hierarchy (currently only the nouns), in which each node of the hierarchy is depicted by hundreds and thousands of images. The project has been instrumental in advancing computer vision and deep learning research. The data is available for free to researchers for non-commercial use\n\nIn above code we have used ResNet 50 architecture to test accuracy, further we will be increasing the model's accuracy using some attention layers\n\nHere, predictions is the output from the model, which is a 2D tensor where each row represents the predicted probabilities for each class for a given example. tf.argmax(predictions, axis=1) returns the index of the maximum value in each row, which corresponds to the predicted class label for each example.","metadata":{}},{"cell_type":"markdown","source":"# ResNet50+Attention","metadata":{}},{"cell_type":"code","source":"\nfrom tensorflow.keras.layers import Dense, Flatten, Input, LayerNormalization, MultiHeadAttention, Add, Dropout,GlobalAveragePooling2D\n\n\n# Load CIFAR-10 dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n# Normalize the data\nx_train = x_train.astype('float32') / 255.0\nx_test = x_test.astype('float32') / 255.0\n\n# Convert class vectors to binary class matrices (one-hot encoding)\ny_train = to_categorical(y_train, 10)\ny_test = to_categorical(y_test, 10)\n\n# Load the ResNet50 model with ImageNet weights, excluding the top layer\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n\n# Add custom layers on top of the base model\nx = base_model.output\nx =GlobalAveragePooling2D()(x)\nprint(x.shape)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T17:07:06.998272Z","iopub.execute_input":"2024-05-20T17:07:06.998997Z","iopub.status.idle":"2024-05-20T17:07:08.959788Z","shell.execute_reply.started":"2024-05-20T17:07:06.998965Z","shell.execute_reply":"2024-05-20T17:07:08.958849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the AttentionBlock\nclass AttentionBlock(tf.keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads):\n        super(AttentionBlock, self).__init__()\n        self.multi_head_attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.norm1 = LayerNormalization(epsilon=1e-6)  # Post-attention normalization\n        self.norm2 = LayerNormalization(epsilon=1e-6)  # Post-dense normalization\n        self.dense = Dense(embed_dim, activation='relu')\n        self.add = Add()\n\n    def build(self, input_shape):\n        # This method can be used to create variables used by the layer\n        super(AttentionBlock, self).build(input_shape)\n\n    def call(self, inputs):\n        attn_output = self.multi_head_attention(inputs, inputs)\n        out1 = self.norm1(self.add([inputs, attn_output]))  # Residual connection + normalization\n        dense_output = self.dense(out1)\n        return self.norm2(self.add([out1, dense_output]))  # Residual connection + normalization\n\n# Set parameters for the AttentionBlock\nembed_dim = 2048  # Match the output dimension of GlobalAveragePooling2D\nnum_heads = 8","metadata":{"execution":{"iopub.status.busy":"2024-05-20T17:07:10.227314Z","iopub.execute_input":"2024-05-20T17:07:10.228181Z","iopub.status.idle":"2024-05-20T17:07:10.236890Z","shell.execute_reply.started":"2024-05-20T17:07:10.228136Z","shell.execute_reply":"2024-05-20T17:07:10.236036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a custom layer to handle dimension expansion and squeezing\nclass ExpandDimsLayer(tf.keras.layers.Layer):\n    def call(self, inputs):\n        return tf.expand_dims(inputs, axis=1)\n\nclass SqueezeDimsLayer(tf.keras.layers.Layer):\n    def call(self, inputs):\n        return tf.squeeze(inputs, axis=1)\n\n# Add the AttentionBlock to the model\nx = ExpandDimsLayer()(x)  # Expand dimensions to add a sequence length of 1\nx = AttentionBlock(embed_dim, num_heads)(x)\nx = SqueezeDimsLayer()(x)  # Squeeze dimensions back to remove the sequence length of 1\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\npredictions = Dense(10, activation='softmax')(x)\n\n# Define the model\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# # Set trainable layers\n# trainable_layers = ['ExpandDimsLayer', 'AttentionBlock', 'SqueezeDimsLayer', 'dense', 'predictions']\n\n# for layer in model.layers:\n#     if layer.name.split('/')[0] in trainable_layers:\n#         layer.trainable = True\n#     else:\n#         layer.trainable = False\n\n\n#Set all layers to be trainable\nfor layer in base_model.layers:\n    layer.trainable = True\n\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n\n# Evaluate the model\ntest_loss, test_accuracy = model.evaluate(x_test, y_test)\nprint(f\"Test accuracy: {test_accuracy}\")\n\n# Predict labels for test set\npredictions = model.predict(x_test)\npredicted_labels = tf.argmax(predictions, axis=1)\nactual_labels = tf.argmax(y_test, axis=1)\n\n# Print overall accuracy\naccuracy = tf.reduce_mean(tf.cast(predicted_labels == actual_labels, tf.float32))\nprint(f\"Overall accuracy: {accuracy.numpy()}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-20T17:07:13.649108Z","iopub.execute_input":"2024-05-20T17:07:13.649886Z","iopub.status.idle":"2024-05-20T17:21:27.545873Z","shell.execute_reply.started":"2024-05-20T17:07:13.649841Z","shell.execute_reply":"2024-05-20T17:21:27.544932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We should avoid using tf.expand_dims and tf.squeeze directly on Keras tensors and instead wrap them within a custom layer or use appropriate Keras layers.\n","metadata":{}},{"cell_type":"markdown","source":"Note that ResNet-50 might be too complex for a relatively small dataset like CIFAR-10, and using a smaller architecture like VGG16 could be more appropriate. We will train the model from scratch since the pretrained weights of imagenet data, may not be very useful to us","metadata":{}},{"cell_type":"markdown","source":"# VGG 16","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.utils import to_categorical\n\n# Load CIFAR-10 dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n# Normalize the data\nx_train = x_train.astype('float32') / 255.0\nx_test = x_test.astype('float32') / 255.0\n\n# Convert class vectors to binary class matrices (one-hot encoding)\ny_train = to_categorical(y_train, 10)\ny_test = to_categorical(y_test, 10)\n\n# Load the VGG16 model with pre-trained ImageNet weights, excluding the top layer\nbase_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n\n# Fine-tune the model by unfreezing some layers\nfor layer in base_model.layers[:-4]:\n    layer.trainable = False\n\n# Add custom layers on top of the base model\nx = base_model.output\nx = Flatten()(x)\nx = Dense(256, activation='relu')(x)\nx = BatchNormalization()(x)\nx = Dropout(0.5)(x)  # Add dropout for regularization\npredictions = Dense(10, activation='softmax')(x)  # 10 classes for CIFAR-10\n\n# Define the model\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(x_train, y_train, epochs=25, batch_size=32, validation_data=(x_test, y_test))\n\n# Evaluate the model\ntest_loss, test_accuracy = model.evaluate(x_test, y_test)\nprint(f\"Test accuracy: {test_accuracy}\")\n\n# Predict labels for test set\npredictions = model.predict(x_test)\npredicted_labels = tf.argmax(predictions, axis=1)\nactual_labels = tf.argmax(y_test, axis=1)\n\n# Print overall accuracy\naccuracy = tf.reduce_mean(tf.cast(predicted_labels == actual_labels, tf.float32))\nprint(f\"Overall accuracy: {accuracy.numpy()}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T17:38:17.896917Z","iopub.execute_input":"2024-05-20T17:38:17.897570Z","iopub.status.idle":"2024-05-20T17:44:42.823361Z","shell.execute_reply.started":"2024-05-20T17:38:17.897538Z","shell.execute_reply":"2024-05-20T17:44:42.822393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ResNet9 Architecture","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model, optimizers\nimport numpy as np\n\n\nfrom tensorflow.keras import layers, Sequential\n\n# Custom padding layer\nclass CustomPaddingLayer(layers.Layer):\n    def __init__(self, padding):\n        super(CustomPaddingLayer, self).__init__()\n        self.padding = padding\n\n    def call(self, inputs):\n        return tf.pad(inputs, [[0, 0], [self.padding, self.padding], [self.padding, self.padding], [0, 0]], mode='CONSTANT')\n\n# Define the  layers in TensorFlow\nconv1 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='valid', input_shape=(32, 32, 3)),\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU()\n])\n\ninput_tensor = tf.random.normal([64, 32, 32, 3])  # Assuming the input shape after previous layers\noutput_tensor1 = conv1(input_tensor)\n\nprint(\"Layer after Conv1\",output_tensor1.shape)\n\n\nconv2 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU(),\n    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')  # Max Pooling\n])\n\n\ninput_tensor = tf.random.normal([64, 32, 32, 64])  # Assuming the input shape after previous layers\noutput_tensor1 = conv2(input_tensor)\nprint(\"Layer after Conv2\",output_tensor1.shape)\n\n\nres1_block0 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU()\n])\n\n\ninput_tensor = tf.random.normal([64, 16, 16, 128])  # Assuming the input shape after previous layers\noutput_tensor1 = res1_block0(input_tensor)\nprint(\"Layer after res1_block0\",output_tensor1.shape)\n\n\n\nres1_block1 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU()\n])\n\n\ninput_tensor = tf.random.normal([64, 16, 16, 128])  # Assuming the input shape after previous layers\noutput_tensor1 = res1_block1(input_tensor)\nprint(\"Layer after res1_block1\",output_tensor1.shape)\n\n\nconv3 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(256, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU(),\n    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')  # Max pooling\n])\n\ninput_tensor = tf.random.normal([64, 16, 16, 128])  # Assuming the input shape after previous layers\noutput_tensor1 = conv3(input_tensor)\nprint(\"Layer after Conv3\",output_tensor1.shape)\n\n\nconv4 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU(),\n    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')  # Max pooling\n])\n\ninput_tensor = tf.random.normal([64, 8, 8, 256])  # Assuming the input shape after previous layers\noutput_tensor1 = conv4(input_tensor)\nprint(\"Layer after Conv4\",output_tensor1.shape)\n\n\nres2_block0 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU()\n])\n\ninput_tensor = tf.random.normal([64, 4,4, 512])  # Assuming the input shape after previous layers\noutput_tensor1 = res2_block0(input_tensor)\nprint(\"Layer after res2_block0\",output_tensor1.shape)\n\n\n\nres2_block1 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU()\n])\n\ninput_tensor = tf.random.normal([64, 4,4, 512])  # Assuming the input shape after previous layers\noutput_tensor1 = res2_block1(input_tensor)\nprint(\"Layer after res2_block1\",output_tensor1.shape)\n\n\n\nclassifier = Sequential([\n    layers.MaxPooling2D(pool_size=(2, 2), strides=1, padding='valid'),  # Max pooling\n    layers.Flatten(),  # Flatten layer\n    layers.Dropout(0.2),  # Dropout with a dropout rate of 0.2\n    layers.Dense(10, activation='softmax')  # Fully connected layer with softmax activation for classification\n])\n\n# Example input to test the layer\ninput_tensor = tf.random.normal([64, 4, 4, 512])  # Assuming the input shape after previous layers\noutput_tensor = classifier(input_tensor)\n\nprint(\"classifier_layer\",output_tensor.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T07:30:51.407855Z","iopub.execute_input":"2024-05-21T07:30:51.408890Z","iopub.status.idle":"2024-05-21T07:30:52.031139Z","shell.execute_reply.started":"2024-05-21T07:30:51.408855Z","shell.execute_reply":"2024-05-21T07:30:52.030250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import Sequential\n\n# Create a Sequential model\nmodel = Sequential()\n\n# Add layers to the model sequentially\nmodel.add(conv1)\nmodel.add(conv2)\nmodel.add(res1_block0)\nmodel.add(res1_block1)\nmodel.add(conv3)\nmodel.add(conv4)\nmodel.add(res2_block0)\nmodel.add(res2_block1)\nmodel.add(classifier)\n\n# Print model summary\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T07:30:58.104044Z","iopub.execute_input":"2024-05-21T07:30:58.104741Z","iopub.status.idle":"2024-05-21T07:30:58.132002Z","shell.execute_reply.started":"2024-05-21T07:30:58.104709Z","shell.execute_reply":"2024-05-21T07:30:58.131212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.datasets import cifar10\n\n# Load CIFAR-10 dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nx_train = x_train.astype('float32') / 255.0\nx_test = x_test.astype('float32') / 255.0\n\n\n# Define data augmentation\ndata_augmentation = Sequential([\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.1),\n])\n\n# Compile the model\nmodel = Sequential([\n    data_augmentation,\n    conv1, conv2, res1_block0, res1_block1, conv3, conv4, res2_block0, res2_block1, classifier\n])\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(x_train, y_train, batch_size=64, epochs=20, validation_data=(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2024-05-21T07:31:14.386952Z","iopub.execute_input":"2024-05-21T07:31:14.387721Z","iopub.status.idle":"2024-05-21T07:41:39.966685Z","shell.execute_reply.started":"2024-05-21T07:31:14.387690Z","shell.execute_reply":"2024-05-21T07:41:39.965709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ResNet9+Attention","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model, optimizers\nfrom tensorflow.keras.layers import Dense, Flatten, Input, LayerNormalization, MultiHeadAttention, Add, Dropout,GlobalAveragePooling2D\nimport numpy as np\n\n\nfrom tensorflow.keras import layers, Sequential\n\n# Custom padding layer\nclass CustomPaddingLayer(layers.Layer):\n    def __init__(self, padding):\n        super(CustomPaddingLayer, self).__init__()\n        self.padding = padding\n\n    def call(self, inputs):\n        return tf.pad(inputs, [[0, 0], [self.padding, self.padding], [self.padding, self.padding], [0, 0]], mode='CONSTANT')\n\n# Define the  layers in TensorFlow\nconv1 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='valid', input_shape=(32, 32, 3)),\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU()\n])\n\ninput_tensor = tf.random.normal([64, 32, 32, 3])  # Assuming the input shape after previous layers\noutput_tensor1 = conv1(input_tensor)\n\nprint(\"Layer after Conv1\",output_tensor1.shape)\n\n\nconv2 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU(),\n    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')  # Max Pooling\n])\n\n\ninput_tensor = tf.random.normal([64, 32, 32, 64])  # Assuming the input shape after previous layers\noutput_tensor1 = conv2(input_tensor)\nprint(\"Layer after Conv2\",output_tensor1.shape)\n\n\nres1_block0 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU()\n])\n\n\ninput_tensor = tf.random.normal([64, 16, 16, 128])  # Assuming the input shape after previous layers\noutput_tensor1 = res1_block0(input_tensor)\nprint(\"Layer after res1_block0\",output_tensor1.shape)\n\n\n\nres1_block1 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU()\n])\n\n\ninput_tensor = tf.random.normal([64, 16, 16, 128])  # Assuming the input shape after previous layers\noutput_tensor1 = res1_block1(input_tensor)\nprint(\"Layer after res1_block1\",output_tensor1.shape)\n\n\nconv3 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(256, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU(),\n    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')  # Max pooling\n])\n\ninput_tensor = tf.random.normal([64, 16, 16, 128])  # Assuming the input shape after previous layers\noutput_tensor1 = conv3(input_tensor)\nprint(\"Layer after Conv3\",output_tensor1.shape)\n\n\nconv4 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU(),\n    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')  # Max pooling\n])\n\ninput_tensor = tf.random.normal([64, 8, 8, 256])  # Assuming the input shape after previous layers\noutput_tensor1 = conv4(input_tensor)\nprint(\"Layer after Conv4\",output_tensor1.shape)\n\n\nres2_block0 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU()\n])\n\ninput_tensor = tf.random.normal([64, 4,4, 512])  # Assuming the input shape after previous layers\noutput_tensor1 = res2_block0(input_tensor)\nprint(\"Layer after res2_block0\",output_tensor1.shape)\n\n\n\nres2_block1 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU()\n])\n\ninput_tensor = tf.random.normal([64, 4,4, 512])  # Assuming the input shape after previous layers\noutput_tensor1 = res2_block1(input_tensor)\nprint(\"Layer after res2_block1\",output_tensor1.shape)\n\n\n\nclassifier = Sequential([\n    layers.MaxPooling2D(pool_size=(2, 2), strides=1, padding='valid'),  # Max pooling\n    layers.Flatten(),  # Flatten layer\n    layers.Dropout(0.2),  # Dropout with a dropout rate of 0.2\n    layers.Dense(10, activation='softmax')  # Fully connected layer with softmax activation for classification\n])\n\n# Example input to test the layer\ninput_tensor = tf.random.normal([64, 4, 4, 512])  # Assuming the input shape after previous layers\noutput_tensor = classifier(input_tensor)\n\nprint(\"classifier_layer\",output_tensor.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T07:41:39.968642Z","iopub.execute_input":"2024-05-21T07:41:39.968949Z","iopub.status.idle":"2024-05-21T07:41:40.241609Z","shell.execute_reply.started":"2024-05-21T07:41:39.968923Z","shell.execute_reply":"2024-05-21T07:41:40.240645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Define the AttentionBlock\nclass AttentionBlock(tf.keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads):\n        super(AttentionBlock, self).__init__()\n        self.multi_head_attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.norm1 = LayerNormalization(epsilon=1e-6)  # Post-attention normalization\n        self.norm2 = LayerNormalization(epsilon=1e-6)  # Post-dense normalization\n        self.dense = Dense(embed_dim, activation='relu')\n        self.add = Add()\n\n    def build(self, input_shape):\n        # This method can be used to create variables used by the layer\n        super(AttentionBlock, self).build(input_shape)\n\n    def call(self, inputs):\n        attn_output = self.multi_head_attention(inputs, inputs)\n        out1 = self.norm1(self.add([inputs, attn_output]))  # Residual connection + normalization\n        dense_output = self.dense(out1)\n        return self.norm2(self.add([out1, dense_output]))  # Residual connection + normalization\n\n# Set parameters for the AttentionBlock\nembed_dim = 2048  # Match the output dimension of GlobalAveragePooling2D\nnum_heads = 8","metadata":{"execution":{"iopub.status.busy":"2024-05-21T07:41:40.242844Z","iopub.execute_input":"2024-05-21T07:41:40.243217Z","iopub.status.idle":"2024-05-21T07:41:40.252057Z","shell.execute_reply.started":"2024-05-21T07:41:40.243165Z","shell.execute_reply":"2024-05-21T07:41:40.251119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import Sequential\n\n# Create a Sequential model\nmodel = Sequential()\n\n# Add layers to the model sequentially\nmodel.add(conv1)\nmodel.add(conv2)\n\nmodel.add(res1_block0)\n\n# Add the AttentionBlock here\nattention_block = AttentionBlock(embed_dim, num_heads)\nmodel.add(attention_block)\n\nmodel.add(res1_block1)\n\n# Add the AttentionBlock here\nattention_block = AttentionBlock(embed_dim, num_heads)\nmodel.add(attention_block)\n\nmodel.add(conv3)\nmodel.add(conv4)\n\nmodel.add(res2_block0)\n\n# Add the AttentionBlock here\nattention_block = AttentionBlock(embed_dim, num_heads)\nmodel.add(attention_block)\n\nmodel.add(res2_block1)\n\n# Add the AttentionBlock here\nattention_block = AttentionBlock(embed_dim, num_heads)\nmodel.add(attention_block)\n\nmodel.add(classifier)\n\n# Print model summary\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T07:41:40.254767Z","iopub.execute_input":"2024-05-21T07:41:40.255479Z","iopub.status.idle":"2024-05-21T07:41:40.309927Z","shell.execute_reply.started":"2024-05-21T07:41:40.255453Z","shell.execute_reply":"2024-05-21T07:41:40.309123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.datasets import cifar10\n\n# Load CIFAR-10 dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nx_train = x_train.astype('float32') / 255.0\nx_test = x_test.astype('float32') / 255.0\n\n\n# Define data augmentation\ndata_augmentation = Sequential([\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.1),\n])\n\n# Compile the model\nmodel = Sequential([\n    data_augmentation,\n    conv1, conv2, res1_block0, res1_block1, conv3, conv4, res2_block0, res2_block1, classifier\n])\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(x_train, y_train, batch_size=64, epochs=20, validation_data=(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2024-05-21T07:41:40.310844Z","iopub.execute_input":"2024-05-21T07:41:40.311087Z","iopub.status.idle":"2024-05-21T07:52:20.224494Z","shell.execute_reply.started":"2024-05-21T07:41:40.311065Z","shell.execute_reply":"2024-05-21T07:52:20.223572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# This time we will train the model without data augmentation","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model, optimizers\nimport numpy as np\n\n\nfrom tensorflow.keras import layers, Sequential\n\n# Custom padding layer\nclass CustomPaddingLayer(layers.Layer):\n    def __init__(self, padding):\n        super(CustomPaddingLayer, self).__init__()\n        self.padding = padding\n\n    def call(self, inputs):\n        return tf.pad(inputs, [[0, 0], [self.padding, self.padding], [self.padding, self.padding], [0, 0]], mode='CONSTANT')\n\n# Define the  layers in TensorFlow\nconv1 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='valid', input_shape=(32, 32, 3)),\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU()\n])\n\ninput_tensor = tf.random.normal([64, 32, 32, 3])  # Assuming the input shape after previous layers\noutput_tensor1 = conv1(input_tensor)\n\nprint(\"Layer after Conv1\",output_tensor1.shape)\n\n\nconv2 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU(),\n    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')  # Max Pooling\n])\n\n\ninput_tensor = tf.random.normal([64, 32, 32, 64])  # Assuming the input shape after previous layers\noutput_tensor1 = conv2(input_tensor)\nprint(\"Layer after Conv2\",output_tensor1.shape)\n\n\nres1_block0 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU()\n])\n\n\ninput_tensor = tf.random.normal([64, 16, 16, 128])  # Assuming the input shape after previous layers\noutput_tensor1 = res1_block0(input_tensor)\nprint(\"Layer after res1_block0\",output_tensor1.shape)\n\n\n\nres1_block1 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU()\n])\n\n\ninput_tensor = tf.random.normal([64, 16, 16, 128])  # Assuming the input shape after previous layers\noutput_tensor1 = res1_block1(input_tensor)\nprint(\"Layer after res1_block1\",output_tensor1.shape)\n\n\nconv3 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(256, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU(),\n    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')  # Max pooling\n])\n\ninput_tensor = tf.random.normal([64, 16, 16, 128])  # Assuming the input shape after previous layers\noutput_tensor1 = conv3(input_tensor)\nprint(\"Layer after Conv3\",output_tensor1.shape)\n\n\nconv4 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU(),\n    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')  # Max pooling\n])\n\ninput_tensor = tf.random.normal([64, 8, 8, 256])  # Assuming the input shape after previous layers\noutput_tensor1 = conv4(input_tensor)\nprint(\"Layer after Conv4\",output_tensor1.shape)\n\n\nres2_block0 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU()\n])\n\ninput_tensor = tf.random.normal([64, 4,4, 512])  # Assuming the input shape after previous layers\noutput_tensor1 = res2_block0(input_tensor)\nprint(\"Layer after res2_block0\",output_tensor1.shape)\n\n\n\nres2_block1 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU()\n])\n\ninput_tensor = tf.random.normal([64, 4,4, 512])  # Assuming the input shape after previous layers\noutput_tensor1 = res2_block1(input_tensor)\nprint(\"Layer after res2_block1\",output_tensor1.shape)\n\n\n\nclassifier = Sequential([\n    layers.MaxPooling2D(pool_size=(2, 2), strides=1, padding='valid'),  # Max pooling\n    layers.Flatten(),  # Flatten layer\n    layers.Dropout(0.2),  # Dropout with a dropout rate of 0.2\n    layers.Dense(10, activation='softmax')  # Fully connected layer with softmax activation for classification\n])\n\n# Example input to test the layer\ninput_tensor = tf.random.normal([64, 4, 4, 512])  # Assuming the input shape after previous layers\noutput_tensor = classifier(input_tensor)\n\nprint(\"classifier_layer\",output_tensor.shape)\n\nfrom tensorflow.keras import Sequential\n\n# Create a Sequential model\nmodel = Sequential()\n\n# Add layers to the model sequentially\nmodel.add(conv1)\nmodel.add(conv2)\nmodel.add(res1_block0)\nmodel.add(res1_block1)\nmodel.add(conv3)\nmodel.add(conv4)\nmodel.add(res2_block0)\nmodel.add(res2_block1)\nmodel.add(classifier)\n\n# Print model summary\nmodel.summary()\n\nfrom tensorflow.keras.datasets import cifar10\n\n# Load CIFAR-10 dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nx_train = x_train.astype('float32') / 255.0\nx_test = x_test.astype('float32') / 255.0\n\n\n\n\n# Compile the model\nmodel = Sequential([\n    \n    conv1, conv2, res1_block0, res1_block1, conv3, conv4, res2_block0, res2_block1, classifier\n])\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(x_train, y_train, batch_size=64, epochs=20, validation_data=(x_test, y_test))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T08:36:11.336064Z","iopub.execute_input":"2024-05-21T08:36:11.336574Z","iopub.status.idle":"2024-05-21T08:42:25.250779Z","shell.execute_reply.started":"2024-05-21T08:36:11.336542Z","shell.execute_reply":"2024-05-21T08:42:25.249769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model, optimizers\nfrom tensorflow.keras.layers import Dense, Flatten, Input, LayerNormalization, MultiHeadAttention, Add, Dropout,GlobalAveragePooling2D\nimport numpy as np\n\n\nfrom tensorflow.keras import layers, Sequential\n\n# Custom padding layer\nclass CustomPaddingLayer(layers.Layer):\n    def __init__(self, padding):\n        super(CustomPaddingLayer, self).__init__()\n        self.padding = padding\n\n    def call(self, inputs):\n        return tf.pad(inputs, [[0, 0], [self.padding, self.padding], [self.padding, self.padding], [0, 0]], mode='CONSTANT')\n\n# Define the  layers in TensorFlow\nconv1 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='valid', input_shape=(32, 32, 3)),\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU()\n])\n\ninput_tensor = tf.random.normal([64, 32, 32, 3])  # Assuming the input shape after previous layers\noutput_tensor1 = conv1(input_tensor)\n\nprint(\"Layer after Conv1\",output_tensor1.shape)\n\n\nconv2 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU(),\n    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')  # Max Pooling\n])\n\n\ninput_tensor = tf.random.normal([64, 32, 32, 64])  # Assuming the input shape after previous layers\noutput_tensor1 = conv2(input_tensor)\nprint(\"Layer after Conv2\",output_tensor1.shape)\n\n\nres1_block0 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU()\n])\n\n\ninput_tensor = tf.random.normal([64, 16, 16, 128])  # Assuming the input shape after previous layers\noutput_tensor1 = res1_block0(input_tensor)\nprint(\"Layer after res1_block0\",output_tensor1.shape)\n\n\n\nres1_block1 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU()\n])\n\n\ninput_tensor = tf.random.normal([64, 16, 16, 128])  # Assuming the input shape after previous layers\noutput_tensor1 = res1_block1(input_tensor)\nprint(\"Layer after res1_block1\",output_tensor1.shape)\n\n\nconv3 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(256, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU(),\n    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')  # Max pooling\n])\n\ninput_tensor = tf.random.normal([64, 16, 16, 128])  # Assuming the input shape after previous layers\noutput_tensor1 = conv3(input_tensor)\nprint(\"Layer after Conv3\",output_tensor1.shape)\n\n\nconv4 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU(),\n    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')  # Max pooling\n])\n\ninput_tensor = tf.random.normal([64, 8, 8, 256])  # Assuming the input shape after previous layers\noutput_tensor1 = conv4(input_tensor)\nprint(\"Layer after Conv4\",output_tensor1.shape)\n\n\nres2_block0 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU()\n])\n\ninput_tensor = tf.random.normal([64, 4,4, 512])  # Assuming the input shape after previous layers\noutput_tensor1 = res2_block0(input_tensor)\nprint(\"Layer after res2_block0\",output_tensor1.shape)\n\n\n\nres2_block1 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU()\n])\n\ninput_tensor = tf.random.normal([64, 4,4, 512])  # Assuming the input shape after previous layers\noutput_tensor1 = res2_block1(input_tensor)\nprint(\"Layer after res2_block1\",output_tensor1.shape)\n\n\n\nclassifier = Sequential([\n    layers.MaxPooling2D(pool_size=(2, 2), strides=1, padding='valid'),  # Max pooling\n    layers.Flatten(),  # Flatten layer\n    layers.Dropout(0.2),  # Dropout with a dropout rate of 0.2\n    layers.Dense(10, activation='softmax')  # Fully connected layer with softmax activation for classification\n])\n\n# Example input to test the layer\ninput_tensor = tf.random.normal([64, 4, 4, 512])  # Assuming the input shape after previous layers\noutput_tensor = classifier(input_tensor)\n\nprint(\"classifier_layer\",output_tensor.shape)\n\n\n\n\n# Define the AttentionBlock\nclass AttentionBlock(tf.keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads):\n        super(AttentionBlock, self).__init__()\n        self.multi_head_attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.norm1 = LayerNormalization(epsilon=1e-6)  # Post-attention normalization\n        self.norm2 = LayerNormalization(epsilon=1e-6)  # Post-dense normalization\n        self.dense = Dense(embed_dim, activation='relu')\n        self.add = Add()\n\n    def build(self, input_shape):\n        # This method can be used to create variables used by the layer\n        super(AttentionBlock, self).build(input_shape)\n\n    def call(self, inputs):\n        attn_output = self.multi_head_attention(inputs, inputs)\n        out1 = self.norm1(self.add([inputs, attn_output]))  # Residual connection + normalization\n        dense_output = self.dense(out1)\n        return self.norm2(self.add([out1, dense_output]))  # Residual connection + normalization\n\n# Set parameters for the AttentionBlock\nembed_dim = 2048  # Match the output dimension of GlobalAveragePooling2D\nnum_heads = 8\n\n\nfrom tensorflow.keras import Sequential\n\n# Create a Sequential model\nmodel = Sequential()\n\n# Add layers to the model sequentially\nmodel.add(conv1)\nmodel.add(conv2)\n\nmodel.add(res1_block0)\n\n# Add the AttentionBlock here\nattention_block = AttentionBlock(embed_dim, num_heads)\nmodel.add(attention_block)\n\nmodel.add(res1_block1)\n\n# Add the AttentionBlock here\nattention_block = AttentionBlock(embed_dim, num_heads)\nmodel.add(attention_block)\n\nmodel.add(conv3)\nmodel.add(conv4)\n\nmodel.add(res2_block0)\n\n# Add the AttentionBlock here\nattention_block = AttentionBlock(embed_dim, num_heads)\nmodel.add(attention_block)\n\nmodel.add(res2_block1)\n\n# Add the AttentionBlock here\nattention_block = AttentionBlock(embed_dim, num_heads)\nmodel.add(attention_block)\n\nmodel.add(classifier)\n\n# Print model summary\nmodel.summary()\n\n\nfrom tensorflow.keras.datasets import cifar10\n\n# Load CIFAR-10 dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nx_train = x_train.astype('float32') / 255.0\nx_test = x_test.astype('float32') / 255.0\n\n\n\n\n# Compile the model\nmodel = Sequential([\n   \n    conv1, conv2, res1_block0, res1_block1, conv3, conv4, res2_block0, res2_block1, classifier\n])\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(x_train, y_train, batch_size=64, epochs=20, validation_data=(x_test, y_test))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T08:42:25.252426Z","iopub.execute_input":"2024-05-21T08:42:25.252719Z","iopub.status.idle":"2024-05-21T08:48:45.917217Z","shell.execute_reply.started":"2024-05-21T08:42:25.252693Z","shell.execute_reply":"2024-05-21T08:48:45.916201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Normal CNN without residual blocks and then using attention layers on top of it","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model, optimizers\nfrom tensorflow.keras.layers import Dense, Flatten, Input, LayerNormalization, MultiHeadAttention, Add, Dropout,GlobalAveragePooling2D\nimport numpy as np\n\n\nfrom tensorflow.keras import layers, Sequential\n\n# Custom padding layer\nclass CustomPaddingLayer(layers.Layer):\n    def __init__(self, padding):\n        super(CustomPaddingLayer, self).__init__()\n        self.padding = padding\n\n    def call(self, inputs):\n        return tf.pad(inputs, [[0, 0], [self.padding, self.padding], [self.padding, self.padding], [0, 0]], mode='CONSTANT')\n\n# Define the  layers in TensorFlow\nconv1 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='valid', input_shape=(32, 32, 3)),\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU()\n])\n\ninput_tensor = tf.random.normal([64, 32, 32, 3])  # Assuming the input shape after previous layers\noutput_tensor1 = conv1(input_tensor)\n\nprint(\"Layer after Conv1\",output_tensor1.shape)\n\n\nconv2 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU(),\n    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')  # Max Pooling\n])\n\n\ninput_tensor = tf.random.normal([64, 32, 32, 64])  # Assuming the input shape after previous layers\noutput_tensor1 = conv2(input_tensor)\nprint(\"Layer after Conv2\",output_tensor1.shape)\n\n\n\nconv3 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(256, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU(),\n    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')  # Max pooling\n])\n\ninput_tensor = tf.random.normal([64, 16, 16, 128])  # Assuming the input shape after previous layers\noutput_tensor1 = conv3(input_tensor)\nprint(\"Layer after Conv3\",output_tensor1.shape)\n\n\nconv4 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU(),\n    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')  # Max pooling\n])\n\ninput_tensor = tf.random.normal([64, 8, 8, 256])  # Assuming the input shape after previous layers\noutput_tensor1 = conv4(input_tensor)\nprint(\"Layer after Conv4\",output_tensor1.shape)\n\n\n\nclassifier = Sequential([\n    layers.MaxPooling2D(pool_size=(2, 2), strides=1, padding='valid'),  # Max pooling\n    layers.Flatten(),  # Flatten layer\n    layers.Dropout(0.2),  # Dropout with a dropout rate of 0.2\n    layers.Dense(10, activation='softmax')  # Fully connected layer with softmax activation for classification\n])\n\n# Example input to test the layer\ninput_tensor = tf.random.normal([64, 4, 4, 512])  # Assuming the input shape after previous layers\noutput_tensor = classifier(input_tensor)\n\nprint(\"classifier_layer\",output_tensor.shape)\n\n\n\n\n# # Define the AttentionBlock\n# class AttentionBlock(tf.keras.layers.Layer):\n#     def __init__(self, embed_dim, num_heads):\n#         super(AttentionBlock, self).__init__()\n#         self.multi_head_attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n#         self.norm1 = LayerNormalization(epsilon=1e-6)  # Post-attention normalization\n#         self.norm2 = LayerNormalization(epsilon=1e-6)  # Post-dense normalization\n#         self.dense = Dense(embed_dim, activation='relu')\n#         self.add = Add()\n\n#     def build(self, input_shape):\n#         # This method can be used to create variables used by the layer\n#         super(AttentionBlock, self).build(input_shape)\n\n#     def call(self, inputs):\n#         attn_output = self.multi_head_attention(inputs, inputs)\n#         out1 = self.norm1(self.add([inputs, attn_output]))  # Residual connection + normalization\n#         dense_output = self.dense(out1)\n#         return self.norm2(self.add([out1, dense_output]))  # Residual connection + normalization\n\n# # Set parameters for the AttentionBlock\n# embed_dim = 2048  # Match the output dimension of GlobalAveragePooling2D\n# num_heads = 8\n\n\nfrom tensorflow.keras import Sequential\n\n# Create a Sequential model\nmodel = Sequential()\n\n# Add layers to the model sequentially\nmodel.add(conv1)\nmodel.add(conv2)\n\n# # Add the AttentionBlock here\n# attention_block = AttentionBlock(embed_dim, num_heads)\n# model.add(attention_block)\n\n# # Add the AttentionBlock here\n# attention_block = AttentionBlock(embed_dim, num_heads)\n# model.add(attention_block)\n\nmodel.add(conv3)\nmodel.add(conv4)\n\n\n\n# # Add the AttentionBlock here\n# attention_block = AttentionBlock(embed_dim, num_heads)\n# model.add(attention_block)\n\n\n\n# # Add the AttentionBlock here\n# attention_block = AttentionBlock(embed_dim, num_heads)\n# model.add(attention_block)\n\nmodel.add(classifier)\n\n# Print model summary\nmodel.summary()\n\n\nfrom tensorflow.keras.datasets import cifar10\n\n# Load CIFAR-10 dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nx_train = x_train.astype('float32') / 255.0\nx_test = x_test.astype('float32') / 255.0\n\n\n\n\n# Compile the model\nmodel = Sequential([\n   \n    conv1, conv2,  conv3, conv4,  classifier\n])\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(x_train, y_train, batch_size=64, epochs=20, validation_data=(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2024-05-21T09:50:25.355587Z","iopub.execute_input":"2024-05-21T09:50:25.355935Z","iopub.status.idle":"2024-05-21T09:54:27.255823Z","shell.execute_reply.started":"2024-05-21T09:50:25.355906Z","shell.execute_reply":"2024-05-21T09:54:27.254860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model, optimizers\nfrom tensorflow.keras.layers import Dense, Flatten, Input, LayerNormalization, MultiHeadAttention, Add, Dropout,GlobalAveragePooling2D\nimport numpy as np\n\n\nfrom tensorflow.keras import layers, Sequential\n\n# Custom padding layer\nclass CustomPaddingLayer(layers.Layer):\n    def __init__(self, padding):\n        super(CustomPaddingLayer, self).__init__()\n        self.padding = padding\n\n    def call(self, inputs):\n        return tf.pad(inputs, [[0, 0], [self.padding, self.padding], [self.padding, self.padding], [0, 0]], mode='CONSTANT')\n\n# Define the  layers in TensorFlow\nconv1 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='valid', input_shape=(32, 32, 3)),\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU()\n])\n\ninput_tensor = tf.random.normal([64, 32, 32, 3])  # Assuming the input shape after previous layers\noutput_tensor1 = conv1(input_tensor)\n\nprint(\"Layer after Conv1\",output_tensor1.shape)\n\n\nconv2 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU(),\n    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')  # Max Pooling\n])\n\n\ninput_tensor = tf.random.normal([64, 32, 32, 64])  # Assuming the input shape after previous layers\noutput_tensor1 = conv2(input_tensor)\nprint(\"Layer after Conv2\",output_tensor1.shape)\n\n\n\nconv3 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(256, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU(),\n    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')  # Max pooling\n])\n\ninput_tensor = tf.random.normal([64, 16, 16, 128])  # Assuming the input shape after previous layers\noutput_tensor1 = conv3(input_tensor)\nprint(\"Layer after Conv3\",output_tensor1.shape)\n\n\nconv4 = Sequential([\n    CustomPaddingLayer(padding=1),  # Adds padding of 1 on each side\n    layers.Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding='valid'),  # Convolution with padding already handled\n    layers.BatchNormalization(epsilon=1e-05, momentum=0.1),\n    layers.ReLU(),\n    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')  # Max pooling\n])\n\ninput_tensor = tf.random.normal([64, 8, 8, 256])  # Assuming the input shape after previous layers\noutput_tensor1 = conv4(input_tensor)\nprint(\"Layer after Conv4\",output_tensor1.shape)\n\n\n\nclassifier = Sequential([\n    layers.MaxPooling2D(pool_size=(2, 2), strides=1, padding='valid'),  # Max pooling\n    layers.Flatten(),  # Flatten layer\n    layers.Dropout(0.2),  # Dropout with a dropout rate of 0.2\n    layers.Dense(10, activation='softmax')  # Fully connected layer with softmax activation for classification\n])\n\n# Example input to test the layer\ninput_tensor = tf.random.normal([64, 4, 4, 512])  # Assuming the input shape after previous layers\noutput_tensor = classifier(input_tensor)\n\nprint(\"classifier_layer\",output_tensor.shape)\n\n\n\n\n# Define the AttentionBlock\nclass AttentionBlock(tf.keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads):\n        super(AttentionBlock, self).__init__()\n        self.multi_head_attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.norm1 = LayerNormalization(epsilon=1e-6)  # Post-attention normalization\n        self.norm2 = LayerNormalization(epsilon=1e-6)  # Post-dense normalization\n        self.dense = Dense(embed_dim, activation='relu')\n        self.add = Add()\n\n    def build(self, input_shape):\n        # This method can be used to create variables used by the layer\n        super(AttentionBlock, self).build(input_shape)\n\n    def call(self, inputs):\n        attn_output = self.multi_head_attention(inputs, inputs)\n        out1 = self.norm1(self.add([inputs, attn_output]))  # Residual connection + normalization\n        dense_output = self.dense(out1)\n        return self.norm2(self.add([out1, dense_output]))  # Residual connection + normalization\n\n# Set parameters for the AttentionBlock\nembed_dim = 2048  # Match the output dimension of GlobalAveragePooling2D\nnum_heads = 8\n\n\nfrom tensorflow.keras import Sequential\n\n# Create a Sequential model\nmodel = Sequential()\n\n# Add layers to the model sequentially\nmodel.add(conv1)\nmodel.add(conv2)\n\n# Add the AttentionBlock here\nattention_block = AttentionBlock(embed_dim, num_heads)\nmodel.add(attention_block)\n\n# Add the AttentionBlock here\nattention_block = AttentionBlock(embed_dim, num_heads)\nmodel.add(attention_block)\n\nmodel.add(conv3)\nmodel.add(conv4)\n\n\n\n# Add the AttentionBlock here\nattention_block = AttentionBlock(embed_dim, num_heads)\nmodel.add(attention_block)\n\n\n\n# Add the AttentionBlock here\nattention_block = AttentionBlock(embed_dim, num_heads)\nmodel.add(attention_block)\n\nmodel.add(classifier)\n\n# Print model summary\nmodel.summary()\n\n\nfrom tensorflow.keras.datasets import cifar10\n\n# Load CIFAR-10 dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nx_train = x_train.astype('float32') / 255.0\nx_test = x_test.astype('float32') / 255.0\n\n\n\n\n# Compile the model\nmodel = Sequential([\n   \n    conv1, conv2,  conv3, conv4,  classifier\n])\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(x_train, y_train, batch_size=64, epochs=20, validation_data=(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2024-05-21T09:54:27.257500Z","iopub.execute_input":"2024-05-21T09:54:27.258053Z","iopub.status.idle":"2024-05-21T09:58:04.093736Z","shell.execute_reply.started":"2024-05-21T09:54:27.258027Z","shell.execute_reply":"2024-05-21T09:58:04.092838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Understanding Why ViT Trains Badly on Small Datasets: An Intuitive Perspective\nVision transformer (ViT) is an attention neural network architecture that is shown to be effective for computer vision tasks. However, compared to ResNet-18 with a similar number of parameters, ViT has a significantly lower evaluation accuracy when trained on small datasets. To facilitate studies in related fields, we provide a visual intuition to help understand why it is the case. We first compare the performance of the two models and confirm that ViT has less accuracy than ResNet-18 when trained on small datasets. We then interpret the results by showing attention map visualization for ViT and feature map visualization for ResNet-18. The difference is further analyzed through a representation similarity perspective. We conclude that the representation of ViT trained on small datasets is hugely different from ViT trained on large datasets, which may be the reason why the performance drops a lot on small datasets.\n\n\nhttps://arxiv.org/abs/2302.03751\n\nhttps://franky07724-57962.medium.com/once-upon-a-time-in-cifar-10-c26bb056b4ce#:~:text=The%20error%20rate%20of%20a,as%20a%20super%2Dhuman%20performance.","metadata":{}}]}